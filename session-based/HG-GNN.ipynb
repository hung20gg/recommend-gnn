{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq dgl-cu118 dglgo -f https://data.dgl.ai/wheels/cu118/repo.html &>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Timedelta\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import dgl\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as FN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY ='content/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "[Last-FM](https://www.kaggle.com/datasets/japarra27/lastfm-dataset)\n",
    "\n",
    "[Heterogeneous Global Graph Neural Networks for Personalized\n",
    "Session-based Recommendation](https://arxiv.org/pdf/2107.03813.pdf)\n",
    "\n",
    "[Github](https://github.com/0215Arthur/HG-GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19098852"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet(f'{DIRECTORY}/lastfm_union.parquet')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>country</th>\n",
       "      <th>registered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 23:08:57+00:00</td>\n",
       "      <td>f1b1cf71-bd35-4e99-8624-24a6e15f133a</td>\n",
       "      <td>Deep Dish</td>\n",
       "      <td>None</td>\n",
       "      <td>Fuck Me Im Famous (Pacha Ibiza)-09-28-2007</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>2006-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:54:10+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>åæ¬é¾ä¸</td>\n",
       "      <td>None</td>\n",
       "      <td>Composition 0919 (Live_2009_4_15)</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>2006-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:52:04+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>åæ¬é¾ä¸</td>\n",
       "      <td>None</td>\n",
       "      <td>Mc2 (Live_2009_4_15)</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>2006-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:52+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>åæ¬é¾ä¸</td>\n",
       "      <td>None</td>\n",
       "      <td>Hibari (Live_2009_4_15)</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>2006-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_000001</td>\n",
       "      <td>2009-05-04 13:42:11+00:00</td>\n",
       "      <td>a7f7df4a-77d8-4f12-8acd-5c60c93f4de8</td>\n",
       "      <td>åæ¬é¾ä¸</td>\n",
       "      <td>None</td>\n",
       "      <td>Mc1 (Live_2009_4_15)</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JAPAN</td>\n",
       "      <td>2006-08-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                 timestamp  \\\n",
       "0  user_000001 2009-05-04 23:08:57+00:00   \n",
       "1  user_000001 2009-05-04 13:54:10+00:00   \n",
       "2  user_000001 2009-05-04 13:52:04+00:00   \n",
       "3  user_000001 2009-05-04 13:42:52+00:00   \n",
       "4  user_000001 2009-05-04 13:42:11+00:00   \n",
       "\n",
       "                              artist_id   artist_name track_id  \\\n",
       "0  f1b1cf71-bd35-4e99-8624-24a6e15f133a     Deep Dish     None   \n",
       "1  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8  åæ¬é¾ä¸     None   \n",
       "2  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8  åæ¬é¾ä¸     None   \n",
       "3  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8  åæ¬é¾ä¸     None   \n",
       "4  a7f7df4a-77d8-4f12-8acd-5c60c93f4de8  åæ¬é¾ä¸     None   \n",
       "\n",
       "                                   track_name gender  age country registered  \n",
       "0  Fuck Me Im Famous (Pacha Ibiza)-09-28-2007      M  NaN   JAPAN 2006-08-13  \n",
       "1           Composition 0919 (Live_2009_4_15)      M  NaN   JAPAN 2006-08-13  \n",
       "2                        Mc2 (Live_2009_4_15)      M  NaN   JAPAN 2006-08-13  \n",
       "3                     Hibari (Live_2009_4_15)      M  NaN   JAPAN 2006-08-13  \n",
       "4                        Mc1 (Live_2009_4_15)      M  NaN   JAPAN 2006-08-13  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # Unique User : 992, # Unique Artist : 107295, # Unique Track : 1083471\n"
     ]
    }
   ],
   "source": [
    "print(f\" # Unique User : {data['user_id'].nunique()}, # Unique Artist : {data['artist_id'].nunique()}, # Unique Track : {data['track_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session with interval of 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_id(df, session):\n",
    "    # If the next row have different user_id or the time difference is greater than session, then it is a new session\n",
    "    df_prev = df.shift(1)\n",
    "    is_new_session = (df['user_id'] != df_prev['user_id']) | (df['timestamp'] - df_prev['timestamp'] > session)\n",
    "    session_id = is_new_session.cumsum()-1\n",
    "    return session_id\n",
    "\n",
    "def group_session(df, session):\n",
    "    df['session_id'] = get_session_id(df, session)\n",
    "    return df\n",
    "\n",
    "def filter_short_session(df, min_session_length=2):\n",
    "    session_length = df.groupby('session_id').size()\n",
    "    session_length = session_length[session_length >= min_session_length]\n",
    "    return df[df['session_id'].isin(session_length.index)]\n",
    "\n",
    "def filter_infrequent_item(df, min_item_support=5):\n",
    "    item_support = df.groupby('item_id').size()\n",
    "    item_support = item_support[item_support >= min_item_support]\n",
    "    return df[df['item_id'].isin(item_support.index)]\n",
    "\n",
    "def filter_until_ok(df, min_session_length=2, min_item_support=5):\n",
    "    while True:\n",
    "        before = len(df)\n",
    "        df = filter_short_session(df, min_session_length)\n",
    "        df = filter_infrequent_item(df, min_item_support)\n",
    "        after = len(df)\n",
    "        if before == after:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def trucate_session(df, session_length=20, is_sorted=True):\n",
    "    if not is_sorted:\n",
    "        df = df.sort_values(['session_id', 'timestamp'])\n",
    "    item_idx = df.groupby('session_id').cumcount()\n",
    "    return df[item_idx < session_length]\n",
    "\n",
    "def update_id(df, field):\n",
    "    labels = pd.factorize(df[field])[0]\n",
    "    kwargs = {field: labels}\n",
    "    df = df.assign(**kwargs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_immediate_repeats(df):\n",
    "    df_prev = df.shift()\n",
    "    is_not_repeat = (df['session_id'] != df_prev['session_id']) | (df['item_id'] != df_prev['item_id'])\n",
    "    return df[is_not_repeat]\n",
    "    \n",
    "def reorder_sessions(df):\n",
    "    df_endtime  = df.groupby('session_id')['timestamp'].max().sort_value().reset_index()\n",
    "    oid2nid = dict(zip(df_endtime['session_id'], df_endtime.index))\n",
    "    df['session_id'].map(oid2nid, inplace=True)\n",
    "    df.sort_values(['session_id', 'timestamp'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def keep_top_n_items(df, n=40000):\n",
    "    item_support = df.groupby('item_id').size()\n",
    "    top_n_items = item_support.nlargest(n).index\n",
    "    return df[df['item_id'].isin(top_n_items)]\n",
    "\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    endtime  = df.groupby('session_id')['timestamp'].max().sort_values()\n",
    "    num_test = int(len(endtime) * test_size)\n",
    "    test_sessions = endtime.index[-num_test:]\n",
    "    df_train = df[~df['session_id'].isin(test_sessions)]\n",
    "    df_test = df[df['session_id'].isin(test_sessions)]\n",
    "    return df_train, df_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = Timedelta(hours=6)\n",
    "n = 40000\n",
    "\n",
    "data = data[['user_id', 'artist_id', 'timestamp']]\n",
    "data.columns = ['user_id', 'item_id', 'timestamp']\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data = update_id(data, 'user_id')\n",
    "data = update_id(data, 'item_id')\n",
    "\n",
    "data.sort_values(['user_id', 'timestamp'], inplace=True)\n",
    "data = group_session(data, interval)\n",
    "\n",
    "data = remove_immediate_repeats(data)\n",
    "data = trucate_session(data, 20)\n",
    "\n",
    "data = keep_top_n_items(data, n)\n",
    "data = filter_until_ok(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16684</th>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>2006-08-13 13:59:20+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16681</th>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "      <td>2006-08-13 14:17:40+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16680</th>\n",
       "      <td>0</td>\n",
       "      <td>551</td>\n",
       "      <td>2006-08-13 14:19:06+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16679</th>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>2006-08-13 14:23:03+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16677</th>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>2006-08-13 14:55:14+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16676</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2006-08-13 14:59:59+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16675</th>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>2006-08-13 15:05:20+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16673</th>\n",
       "      <td>0</td>\n",
       "      <td>577</td>\n",
       "      <td>2006-08-13 15:12:12+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16672</th>\n",
       "      <td>0</td>\n",
       "      <td>525</td>\n",
       "      <td>2006-08-13 15:17:35+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16671</th>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>2006-08-13 15:23:08+00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id                 timestamp  session_id\n",
       "16684        0      186 2006-08-13 13:59:20+00:00           0\n",
       "16681        0      550 2006-08-13 14:17:40+00:00           0\n",
       "16680        0      551 2006-08-13 14:19:06+00:00           0\n",
       "16679        0      440 2006-08-13 14:23:03+00:00           0\n",
       "16677        0      274 2006-08-13 14:55:14+00:00           0\n",
       "16676        0       72 2006-08-13 14:59:59+00:00           0\n",
       "16675        0      250 2006-08-13 15:05:20+00:00           0\n",
       "16673        0      577 2006-08-13 15:12:12+00:00           0\n",
       "16672        0      525 2006-08-13 15:17:35+00:00           0\n",
       "16671        0      280 2006-08-13 15:23:08+00:00           0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19080583</th>\n",
       "      <td>991</td>\n",
       "      <td>64472</td>\n",
       "      <td>2009-05-02 00:03:08+00:00</td>\n",
       "      <td>427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080569</th>\n",
       "      <td>991</td>\n",
       "      <td>2486</td>\n",
       "      <td>2009-05-02 01:07:00+00:00</td>\n",
       "      <td>427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080560</th>\n",
       "      <td>991</td>\n",
       "      <td>12700</td>\n",
       "      <td>2009-05-02 04:00:17+00:00</td>\n",
       "      <td>427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080550</th>\n",
       "      <td>991</td>\n",
       "      <td>829</td>\n",
       "      <td>2009-05-02 04:34:38+00:00</td>\n",
       "      <td>427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080542</th>\n",
       "      <td>991</td>\n",
       "      <td>3331</td>\n",
       "      <td>2009-05-02 05:14:20+00:00</td>\n",
       "      <td>427539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080520</th>\n",
       "      <td>991</td>\n",
       "      <td>38</td>\n",
       "      <td>2009-05-02 17:33:03+00:00</td>\n",
       "      <td>427540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080499</th>\n",
       "      <td>991</td>\n",
       "      <td>17642</td>\n",
       "      <td>2009-05-02 19:51:15+00:00</td>\n",
       "      <td>427540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080498</th>\n",
       "      <td>991</td>\n",
       "      <td>17805</td>\n",
       "      <td>2009-05-02 19:57:17+00:00</td>\n",
       "      <td>427540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080485</th>\n",
       "      <td>991</td>\n",
       "      <td>38713</td>\n",
       "      <td>2009-05-03 20:28:10+00:00</td>\n",
       "      <td>427542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19080484</th>\n",
       "      <td>991</td>\n",
       "      <td>202</td>\n",
       "      <td>2009-05-04 00:27:31+00:00</td>\n",
       "      <td>427542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  item_id                 timestamp  session_id\n",
       "19080583      991    64472 2009-05-02 00:03:08+00:00      427539\n",
       "19080569      991     2486 2009-05-02 01:07:00+00:00      427539\n",
       "19080560      991    12700 2009-05-02 04:00:17+00:00      427539\n",
       "19080550      991      829 2009-05-02 04:34:38+00:00      427539\n",
       "19080542      991     3331 2009-05-02 05:14:20+00:00      427539\n",
       "19080520      991       38 2009-05-02 17:33:03+00:00      427540\n",
       "19080499      991    17642 2009-05-02 19:51:15+00:00      427540\n",
       "19080498      991    17805 2009-05-02 19:57:17+00:00      427540\n",
       "19080485      991    38713 2009-05-03 20:28:10+00:00      427542\n",
       "19080484      991      202 2009-05-04 00:27:31+00:00      427542"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Users : 989 #Items : 39835 #Sessions : 354770 \n"
     ]
    }
   ],
   "source": [
    "print(f\"#Users : {data['user_id'].nunique()} #Items : {data['item_id'].nunique()} #Sessions : {data['session_id'].nunique()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'{DIRECTORY}/data.csv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aggregate_session(df):\n",
    "    res = []\n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        res += group.groupby('session_id')['item_id'].agg(list).tolist()\n",
    "    return res\n",
    "\n",
    "def _aggregate_df(df):\n",
    "    res = dict()\n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        res[uid] = group.groupby('session_id')['item_id'].agg(list).tolist()\n",
    "    \n",
    "    return res\n",
    "        \n",
    "def split_data(val_ratio = 0.2, test_ratio = 0.2):\n",
    "    data = pd.read_csv('data/data.csv', sep='\\t', names=['user_id', 'item_id', 'timestamp','session_id'])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    \n",
    "    df_train, df_test = train_test_split(data, test_size=test_ratio)\n",
    "    \n",
    "    df_test = df_test[df_test['item_id'].isin(df_train['item_id'].unique())]\n",
    "    df_test = filter_short_session(df_test)\n",
    "\n",
    "\n",
    "    # update itemId\n",
    "    train_itemId_new, uniques = pd.factorize(df_train['item_id'])\n",
    "    df_train = df_train.assign(item_id=train_itemId_new)\n",
    "    oid2nid = {oid: i for i, oid in enumerate(uniques)}\n",
    "    test_itemId_new = df_test['item_id'].map(oid2nid)\n",
    "    df_test = df_test.assign(item_id=test_itemId_new)\n",
    "    \n",
    "    df_train['user_id']+=1\n",
    "    df_train['item_id']+=1\n",
    "    df_test['user_id']+=1\n",
    "    df_test['item_id']+=1\n",
    "    \n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_val = df_test.sample(frac=val_ratio, random_state=42)\n",
    "    part_test = df_test[~df_test.index.isin(df_val.index)]\n",
    "    \n",
    "    with open(f'{DIRECTORY}/train.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_train), f)\n",
    "        \n",
    "    with open(f'{DIRECTORY}/val.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_val), f)\n",
    "        \n",
    "    with open(f'{DIRECTORY}/test.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(part_test), f)\n",
    "        \n",
    "    with open(f'{DIRECTORY}/all_test.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_test), f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.17 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 12\n",
    "SEQ_LEN = 10 ## Window Size to create a training Sequence\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_seq(data_list):\n",
    "    # final_seqs = [(user_id, seq, [next_item])]\n",
    "    uid = []\n",
    "\n",
    "    masks = []\n",
    "    labels =[]\n",
    "    browsed_ids = []\n",
    "    temp_browsed_id = [0 for _ in range(SEQ_LEN)]\n",
    "    pos_idx = []\n",
    "    seq_lens = []\n",
    "    \n",
    "    final_seq = []\n",
    "    \n",
    "    for u in tqdm(data_list):\n",
    "        u_seqs = data_list[u]\n",
    "        for seq in u_seqs:\n",
    "            for i in range(1, len(seq)):\n",
    "                \n",
    "                temp_seq = seq[-i-SEQ_LEN:-i]\n",
    "                len_seq = len(temp_seq)\n",
    "                mask = [1]*len_seq + [0]*(SEQ_LEN-len_seq)\n",
    "                pos_id = [len_seq-1-i for i in range(len_seq)]+ [0]*(SEQ_LEN-len_seq)\n",
    "                browsed_id = temp_browsed_id.copy()\n",
    "                browsed_id[:len_seq] = temp_seq\n",
    "                \n",
    "                masks.append(mask)\n",
    "                pos_idx.append(pos_id)\n",
    "                browsed_ids.append(browsed_id)\n",
    "                labels.append([int(seq[-i])])\n",
    "                uid.append([int(u)])\n",
    "                seq_lens.append(len_seq)\n",
    "                \n",
    "                final_seq.append((u, seq[:-i], [seq[-i]]))\n",
    "        \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    uid = torch.tensor(uid, dtype=torch.long)\n",
    "    masks = torch.tensor(masks, dtype=torch.bool)  \n",
    "    browsed_ids = torch.tensor(browsed_ids, dtype=torch.long)\n",
    "    pos_idx = torch.tensor(pos_idx, dtype=torch.long)\n",
    "    seq_lens = torch.tensor(seq_lens, dtype=torch.long)\n",
    "        \n",
    "    return final_seq, (uid, browsed_ids, masks,seq_lens, pos_idx, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39f282c6ca47cbb6992b1981462c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12545dab323b434f994481be1607d9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(f'{DIRECTORY}/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(f'{DIRECTORY}/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "test_seq, test_data = common_seq(test)\n",
    "train_seq, train_data = common_seq(train)\n",
    "\n",
    "# with open('data/train_seq.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_seq, f)\n",
    "# with open('data/test_seq.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_seq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of Training DataLoader is 5839 & Test DataLoader 1084 \n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=TensorDataset(*train_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=TensorDataset(*test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\" Length of Training DataLoader is {len(train_dataloader)} & Test DataLoader {len(test_dataloader)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for an iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in the example tuple 6 - corresponds to uid | browsed_ids | mask | seq_len | label | pos_idx \n",
      "Size of - \n",
      " User ID Tensor - torch.Size([512, 1]) \n",
      " Sequence of Items - torch.Size([512, 10]) \n",
      " Sequence of Masks - torch.Size([512, 10]) \n",
      " Actual Sequence Length (before padding) - torch.Size([512]) \n",
      " Labels - torch.Size([512, 1]) \n",
      " Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - torch.Size([512, 10]) \n"
     ]
    }
   ],
   "source": [
    "eg = next(iter(train_dataloader))\n",
    "print(f\"Number of elements in the example tuple {len(eg)} - corresponds to uid | browsed_ids | mask | seq_len | label | pos_idx \")\n",
    "\n",
    "print(\"Size of - \")\n",
    "print(f\" User ID Tensor - {eg[0].size()} \") \n",
    "print(f\" Sequence of Items - {eg[1].size()} \") \n",
    "print(f\" Sequence of Masks - {eg[2].size()} \") \n",
    "print(f\" Actual Sequence Length (before padding) - {eg[3].size()} \") \n",
    "print(f\" Labels - {eg[5].size()} \") \n",
    "print(f\" Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - {eg[4].size()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tensor of - \n",
      " User ID - tensor([725]) \n",
      " Sequence of Items - tensor([89]) \n",
      " Sequence of Masks - tensor([273]) \n",
      " Actual Sequence Length (before padding) - tensor([367]) \n",
      " Labels - tensor([913]) \n",
      " Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - tensor([773]) \n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tensor of - \")\n",
    "print(f\" User ID - {eg[0][0]} \") \n",
    "print(f\" Sequence of Items - {eg[0][1]} \") \n",
    "print(f\" Sequence of Masks - {eg[0][2]} \") \n",
    "print(f\" Actual Sequence Length (before padding) - {eg[0][3]} \") \n",
    "print(f\" Labels - {eg[0][5]} \") \n",
    "print(f\" Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - {eg[0][4]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Heterogenous Global Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we have to create 4 type of edges\n",
    "* In and out connection (the prev node and the forward node) of a item in session\n",
    "* User similarity\n",
    "* Item similarity (based on session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_relation(num, sample_size=20):\n",
    "    \n",
    "    adj1 = [dict() for _ in range(num)]\n",
    "    adj2 = [dict() for _ in range(num)]\n",
    "    adj_in = [[] for _ in range(num)]\n",
    "    adj_out = [[] for _ in range(num)]\n",
    "\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    \n",
    "    for u in tqdm(graph):\n",
    "        u_seqs = graph[u]\n",
    "        for seq in u_seqs:\n",
    "            for i in range(1, len(seq)):\n",
    "                if seq[i] not in adj1[seq[i-1]]:\n",
    "                    adj1[seq[i-1]][seq[i]] = 1\n",
    "                else:\n",
    "                    adj1[seq[i-1]][seq[i]] += 1\n",
    "\n",
    "                if seq[i-1] not in adj2[seq[i]]:\n",
    "                    adj2[seq[i]][seq[i-1]] = 1\n",
    "                else:\n",
    "                    adj2[seq[i]][seq[i-1]] += 1\n",
    "                    \n",
    "    weights = [[] for _ in range(num)]\n",
    "    \n",
    "    for t in range(1, num):\n",
    "        x = [v for v in sorted(adj1[t].items(), reverse=True, key=lambda x: x[1])]\n",
    "        adj_out[t] = [v[0] for v in x]\n",
    "\n",
    "    for t in range(1, num):\n",
    "        x = [v for v in sorted(adj2[t].items(), reverse=True, key=lambda x: x[1])]\n",
    "        adj_in[t] = [v[0] for v in x]\n",
    "\n",
    "    # edge sampling \n",
    "    for i in range(1, num):\n",
    "        adj_in[i] = adj_in[i][:sample_size]\n",
    "    for i in range(1, num):\n",
    "        adj_out[i] = adj_out[i][:sample_size]\n",
    "        \n",
    "    print(f\"Items which most frequently lies to the left (previous time step) of Item 1 : {adj_in[1]} \")\n",
    "    print(f\"Items which most frequently lies to the right (next time step) of Item 1 : {adj_out[1]} \")\n",
    "        \n",
    "    with open(f'{DIRECTORY}/adj_in.pkl', 'wb') as f:\n",
    "        pickle.dump(adj_in, f)\n",
    "    \n",
    "    with open(f'{DIRECTORY}/adj_out.pkl', 'wb') as f:\n",
    "        pickle.dump(adj_out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userCF(K=100):\n",
    "    \n",
    "    vid_user = {}\n",
    "    user_sim_matrix ={}\n",
    "    uid_vcount = {}\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        session_data = pickle.load(f)\n",
    "        \n",
    "    for uid in tqdm(session_data):\n",
    "        for seq in session_data[uid]:\n",
    "            uid_vcount[uid] = set()\n",
    "            for vid in seq:\n",
    "                if vid not in vid_user:\n",
    "                    vid_user[vid] = set()\n",
    "                vid_user[vid].add(uid)\n",
    "                uid_vcount[uid].add(vid)\n",
    "                \n",
    "    for vid, users in tqdm(vid_user.items()):\n",
    "        for u in users:\n",
    "            if u not in user_sim_matrix:\n",
    "                user_sim_matrix[u] = dict()\n",
    "            for v in users:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                if v not in user_sim_matrix[u]:\n",
    "                    user_sim_matrix[u][v] = 0\n",
    "                user_sim_matrix[u][v] += 1/len(users)\n",
    "                \n",
    "    for u, related_users in tqdm(user_sim_matrix.items()):\n",
    "        for v, count in related_users.items():\n",
    "            user_sim_matrix[u][v] = count / math.sqrt(len(uid_vcount[u]) * len(uid_vcount[v]))\n",
    "    \n",
    "    user_topK = {}     \n",
    "            \n",
    "    for user in user_sim_matrix:\n",
    "        user_topK[user] = sorted(user_sim_matrix[user].items(), key=itemgetter(1), reverse=True)[:K]\n",
    "        \n",
    "    with open(f'{DIRECTORY}/user_sim_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(user_topK, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemCF_by_Session(K=200):\n",
    "    \"\"\"\n",
    "        calculate item similarity matrix by session (kinda like TF-IDF)\n",
    "    \"\"\"\n",
    "    \n",
    "    sess_item = {}\n",
    "    item_sim_matrix = {}\n",
    "    vid_ucount = {}\n",
    "    sess_cnt = 0\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        session_data = pickle.load(f)\n",
    "        \n",
    "    for uid in tqdm(session_data):\n",
    "        for seq in session_data[uid]:\n",
    "            sess_cnt += 1\n",
    "            sess_item[sess_cnt] = set()\n",
    "            for vid in seq:\n",
    "                sess_item[sess_cnt].add(vid)\n",
    "                if vid not in vid_ucount:\n",
    "                    vid_ucount[vid] = set()\n",
    "                vid_ucount[vid].add(sess_cnt)\n",
    "    \n",
    "    for sess, items in tqdm(sess_item.items()):\n",
    "        for u in items:\n",
    "            if u not in item_sim_matrix:\n",
    "                item_sim_matrix[u] = dict()\n",
    "            for v in items:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                if v not in item_sim_matrix[u]:\n",
    "                    item_sim_matrix[u][v] = 0\n",
    "                item_sim_matrix[u][v] += 1/len(items)\n",
    "                \n",
    "    for u, related_items in tqdm(item_sim_matrix.items()):\n",
    "        for v, count in related_items.items():\n",
    "            item_sim_matrix[u][v] = count / math.sqrt(len(vid_ucount[u]) * len(vid_ucount[v]))\n",
    "            \n",
    "    item_topK = {}\n",
    "    for item in item_sim_matrix:\n",
    "        item_topK[item] = sorted(item_sim_matrix[item].items(), key=itemgetter(1), reverse=True)[:K]\n",
    "        \n",
    "    with open(f'{DIRECTORY}/item_sim_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(item_topK, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6ce77734b140b49be6f0e2b6d4b279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items which most frequently lies to the left (previous time step) of Item 1 : [88, 55, 68, 85, 13, 80, 107, 90] \n",
      "Items which most frequently lies to the right (next time step) of Item 1 : [7, 2, 38, 86, 99, 120, 125] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e360070c91e436da33254af4b27e870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e11146d2144da8b591e75dc9f5bfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8551b3fcdcdb46419300ed2bc3d7e3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dd3ecedbec4e92ae77e4676cd8f541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfdc604de0a478b863143b3b177c01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283816 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2add7214664c18b8071e30621c4867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_relation(num=n,sample_size=SZ)\n",
    "userCF()\n",
    "itemCF_by_Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Heterogenous Global Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uui_graph( topK,add_u=True, add_v=True):\n",
    "    \n",
    "    with open(f'{DIRECTORY}/train.pkl', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    with open(f'{DIRECTORY}/adj_in.pkl', 'rb') as f:\n",
    "        adj_in = pickle.load(f)\n",
    "    with open(f'{DIRECTORY}/adj_out.pkl', 'rb') as f:\n",
    "        adj_out = pickle.load(f)\n",
    "    with open(f'{DIRECTORY}/user_sim_matrix.pkl', 'rb') as f:\n",
    "        user_sim_matrix = pickle.load(f)\n",
    "    with open(f'{DIRECTORY}/item_sim_matrix.pkl', 'rb') as f:\n",
    "        item_sim_matrix = pickle.load(f)\n",
    "        \n",
    "    pre = []\n",
    "    nxt = []\n",
    "    src_v = []\n",
    "    dst_u = []\n",
    "    \n",
    "    for i in range(1, len(adj_in)):\n",
    "        _pre = []\n",
    "        _nxt = []\n",
    "        \n",
    "        for j in adj_in[i]:\n",
    "            _pre.append(i)\n",
    "            _nxt.append(j)\n",
    "        \n",
    "        pre += _pre\n",
    "        nxt += _nxt\n",
    "    \n",
    "    o_pre = []\n",
    "    o_nxt = []\n",
    "    \n",
    "    for i in range(1, len(adj_out)):\n",
    "        _pre = []\n",
    "        _nxt = []\n",
    "        \n",
    "        for j in adj_out[i]:\n",
    "            _pre.append(i)\n",
    "            _nxt.append(j)\n",
    "        \n",
    "        o_pre += _pre\n",
    "        o_nxt += _nxt\n",
    "        \n",
    "    for u in tqdm(graph):\n",
    "        for seq in graph[u]:\n",
    "            pre += seq[:-1]\n",
    "            nxt += seq[1:]\n",
    "            dst_u += [u for _ in seq]\n",
    "            src_v += seq\n",
    "            \n",
    "    topv_src = []\n",
    "    topv_dst = []\n",
    "    \n",
    "    for v in tqdm(item_sim_matrix):\n",
    "        tmp_src =[]\n",
    "        tmp_dst =[]\n",
    "        \n",
    "        exclusion = adj_in[v] + adj_out[v]\n",
    "        for vid, value in item_sim_matrix[v][:topK][:int(len(exclusion))]:\n",
    "            if vid not in exclusion:\n",
    "                tmp_src.append(v)\n",
    "                tmp_dst.append(vid)\n",
    "                \n",
    "        topv_src += tmp_src\n",
    "        topv_dst += tmp_dst\n",
    "        \n",
    "    u_src = []\n",
    "    u_dst = []\n",
    "    \n",
    "    for u in tqdm(user_sim_matrix):\n",
    "        tmp_src =[]\n",
    "        tmp_dst =[]\n",
    "        \n",
    "        for uid, value in user_sim_matrix[u][:topK]:\n",
    "            tmp_src.append(u)\n",
    "            tmp_dst.append(uid)\n",
    "                \n",
    "        u_src += tmp_src\n",
    "        u_dst += tmp_dst\n",
    "        \n",
    "    item_num = max(max(pre), max(nxt)) + 1\n",
    "    user_num = max(max(u_src), max(u_dst)) \n",
    "    \n",
    "    u_src = [i+item_num for i in u_src]\n",
    "    u_dst = [i+item_num for i in u_dst]\n",
    "    dst_u = [i+item_num for i in dst_u]\n",
    "    \n",
    "    pre = torch.tensor(pre, dtype=torch.long, device=device)\n",
    "    nxt = torch.tensor(nxt, dtype=torch.long, device=device)\n",
    "    dst_u = torch.tensor(dst_u, dtype=torch.long, device=device)\n",
    "    src_v = torch.tensor(src_v, dtype=torch.long, device=device)\n",
    "    u_src = torch.tensor(u_src, dtype=torch.long, device=device)\n",
    "    u_dst = torch.tensor(u_dst, dtype=torch.long, device=device)\n",
    "    topv_src = torch.tensor(topv_src, dtype=torch.long, device=device)\n",
    "    topv_dst = torch.tensor(topv_dst, dtype=torch.long, device=device)\n",
    "    \n",
    "    G = dgl.graph((pre,nxt))\n",
    "    G.add_edges(nxt, pre)\n",
    "    G.add_edges(dst_u, src_v)\n",
    "    G.add_edges(src_v, dst_u)\n",
    "    \n",
    "    if add_u:\n",
    "        G.add_edges(u_src, u_dst)\n",
    "        G.add_edges(u_dst, u_src)\n",
    "        \n",
    "    if add_v:\n",
    "        G.add_edges(topv_src, topv_dst)\n",
    "        G.add_edges(topv_dst, topv_src)\n",
    "        \n",
    "    G = dgl.add_self_loop(G)\n",
    "    \n",
    "    return G, item_num,pre, nxt, dst_u, src_v, u_src, u_dst, topv_src, topv_dst   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10296a6e35c4a7c8728021bee4325e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a9e7d1b4df42aca893f20ffdf34fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018679dfd4174182a066872efca6e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/897 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DGLError",
     "evalue": "[01:13:22] C:\\Users\\peizhou\\workspace\\DGL_scripts\\release\\win-64\\dgl\\src\\array\\array.cc:50: Operator Range does not support cuda device.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# G,item_num, pre, nxt, dst_u, src_v, u_src, u_dst, topv_src, topv_dst = uui_graph(topK = 20, add_u = True, add_v = True)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m G\u001b[38;5;241m=\u001b[39m \u001b[43muui_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_u\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_v\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 98\u001b[0m, in \u001b[0;36muui_graph\u001b[1;34m(topK, add_u, add_v)\u001b[0m\n\u001b[0;32m     95\u001b[0m topv_dst \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(topv_dst, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     97\u001b[0m G \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mgraph((pre,nxt))\n\u001b[1;32m---> 98\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m G\u001b[38;5;241m.\u001b[39madd_edges(dst_u, src_v)\n\u001b[0;32m    100\u001b[0m G\u001b[38;5;241m.\u001b[39madd_edges(src_v, dst_u)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\heterograph.py:597\u001b[0m, in \u001b[0;36mDGLGraph.add_edges\u001b[1;34m(self, u, v, data, etype)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c_etype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanonical_etypes:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# the target edge type\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c_etype \u001b[38;5;241m==\u001b[39m (u_type, e_type, v_type):\n\u001b[1;32m--> 597\u001b[0m         old_u, old_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc_etype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m         hgidx \u001b[38;5;241m=\u001b[39m heterograph_index\u001b[38;5;241m.\u001b[39mcreate_unitgraph_from_coo(\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m u_type \u001b[38;5;241m==\u001b[39m v_type \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes(u_type),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m             [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    605\u001b[0m         )\n\u001b[0;32m    606\u001b[0m         relation_graphs\u001b[38;5;241m.\u001b[39mappend(hgidx)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\view.py:179\u001b[0m, in \u001b[0;36mHeteroEdgeView.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all the edges.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mall_edges(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\heterograph.py:3591\u001b[0m, in \u001b[0;36mDGLGraph.all_edges\u001b[1;34m(self, form, order, etype)\u001b[0m\n\u001b[0;32m   3521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_edges\u001b[39m(\u001b[38;5;28mself\u001b[39m, form\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muv\u001b[39m\u001b[38;5;124m\"\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m\"\u001b[39m, etype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3522\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return all edges with the specified edge type.\u001b[39;00m\n\u001b[0;32m   3523\u001b[0m \n\u001b[0;32m   3524\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3589\u001b[0m \u001b[38;5;124;03m    out_edges\u001b[39;00m\n\u001b[0;32m   3590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3591\u001b[0m     src, dst, eid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_etype_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m form \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3593\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m src, dst, eid\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\heterograph_index.py:696\u001b[0m, in \u001b[0;36mHeteroGraphIndex.edges\u001b[1;34m(self, etype, order)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m order \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrcdst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect order to be one of None, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrcdst\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(order)\n\u001b[0;32m    695\u001b[0m     )\n\u001b[1;32m--> 696\u001b[0m edge_array \u001b[38;5;241m=\u001b[39m \u001b[43m_CAPI_DGLHeteroEdges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m src \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(edge_array(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    698\u001b[0m dst \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mfrom_dgl_nd(edge_array(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\_ffi\\_ctypes\\function.py:212\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    210\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DGLValue()\n\u001b[0;32m    211\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m--> 212\u001b[0m \u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDGLFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[0;32m    223\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\_ffi\\base.py:70\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(py_str(_LIB\u001b[38;5;241m.\u001b[39mDGLGetLastError()))\n",
      "\u001b[1;31mDGLError\u001b[0m: [01:13:22] C:\\Users\\peizhou\\workspace\\DGL_scripts\\release\\win-64\\dgl\\src\\array\\array.cc:50: Operator Range does not support cuda device."
     ]
    }
   ],
   "source": [
    "G,item_num, pre, nxt, dst_u, src_v, u_src, u_dst, topv_src, topv_dst = uui_graph(topK = 20, add_u = True, add_v = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG_GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ss](image/hg_gnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HG_GNN(nn.Module):\n",
    "    def __init__(self, G, config, item_num, max_seq_len=10, max_sess=10):\n",
    "        super(HG_GNN, self).__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.G = G.to(self.device)\n",
    "        self.item_num = item_num\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_sess = max_sess\n",
    "        self.config = config\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.em_size = config['embed_size']\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(200, self.em_size)\n",
    "        self.v2e = nn.Embedding(G.number_of_nodes(), self.em_size)\n",
    "        \n",
    "        self.conv1 =dglnn.SAGEConv(self.em_size, self.em_size, 'mean')\n",
    "        \n",
    "        dropout = config['dropout']\n",
    "        \n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sigmoid_concat = nn.Sequential(\n",
    "            nn.Linear(self.em_size*2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.w_1 = nn.Parameter(torch.Tensor(2*self.em_size, self.em_size))\n",
    "        self.w_2 = nn.Parameter(torch.Tensor(self.em_size, 1))\n",
    "        \n",
    "        self.glu1 = nn.Linear(self.em_size, self.em_size)\n",
    "        self.glu2 = nn.Linear(self.em_size, self.em_size, bias=False)\n",
    "        \n",
    "        self.w_3 = nn.Parameter(torch.Tensor(self.em_size, self.em_size))\n",
    "        self.w_4 = nn.Parameter(torch.Tensor(self.em_size, 1))\n",
    "        \n",
    "        self.glu3 = nn.Linear(self.em_size, self.em_size)\n",
    "        self.glu4 = nn.Linear(self.em_size, self.em_size, bias=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "                \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.em_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def compute_hidden_vector(self, hidden, mask, pos_idx):\n",
    "        \n",
    "        mask = mask.unsqueeze(-1)\n",
    "        seq_len = hidden.size(1)\n",
    "        pos_embedding = self.pos_embedding(pos_idx)\n",
    "        tmp = torch.sum(hidden*mask, 1)/torch.sum(mask, 1)\n",
    "        \n",
    "        hs = tmp.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        nh = torch.matmul(torch.cat([pos_embedding,hidden],-1),self.w_1)\n",
    "        nh = torch.tanh(nh)\n",
    "        nh = torch.sigmoid(self.glu1(nh)+self.glu2(hs))\n",
    "        \n",
    "        beta = torch.matmul(nh, self.w_2) * mask\n",
    "        \n",
    "        select = torch.sum(beta*hidden, 1)\n",
    "        \n",
    "        return select, tmp\n",
    "        \n",
    "    def sess_user_vector(self, user_vec, note_embeds, mask):\n",
    "        \n",
    "        mask = mask.unsqueeze(-1)\n",
    "        \n",
    "        hs = user_vec.repeat(1, self.mask.size(1), 1)\n",
    "        nh = torch.matmul(note_embeds,self.w_3)\n",
    "        nh = torch.tanh(nh)\n",
    "        nh = torch.sigmoid(self.glu3(nh)+self.glu4(hs))\n",
    "        \n",
    "        beta = torch.matmul(nh, self.w_4) * mask\n",
    "        select = torch.sum(beta*note_embeds, 1)\n",
    "        \n",
    "        return select\n",
    "        \n",
    "    def forward(self,user, seq, mask, seq_len, pos_idx):\n",
    "        \"\"\"\n",
    "        seq(bs*L)\n",
    "        seq: bs*L\n",
    "        his_ids: bs * M\n",
    "        mask:\n",
    "        seq_len(bs)\n",
    "        \"\"\"\n",
    "        \n",
    "        user = user + self.item_num\n",
    "        v2e_all = self.v2e(torch.arange(0, self.G.number_of_nodes()).long().to(self.device))\n",
    "        h1 = self.conv1(self.G,self.emb_dropout(v2e_all))\n",
    "        h1 = F.relu(h1)\n",
    "        \n",
    "        bs = seq.size(0)\n",
    "        L = seq.size(1)\n",
    "        \n",
    "        node_list = seq\n",
    "        nodes_embeds = (h1[node_list] + self.v2e(node_list))/2\n",
    "        seq_embeds = (h1[user] + self.v2e(user))/2\n",
    "\n",
    "        nodes_embeds = nodes_embeds.view(bs, L, -1)\n",
    "        \n",
    "        sess_vec, avg_sess = self.compute_hidden_vector(nodes_embeds, mask, pos_idx)\n",
    "        sess_user = self.sess_user_vector(seq_embeds, nodes_embeds, mask)\n",
    "        \n",
    "        alpha = self.sigmoid_concat(torch.cat([sess_user, sess_vec], 1))\n",
    "        \n",
    "        seq_embeds = alpha * sess_user + (1-alpha) * sess_vec\n",
    "        scores = torch.matmul(seq_embeds, v2e_all.T)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "'embed_size' : 128,\n",
    "'learning_rate' : 0.001,\n",
    "'hidden_size' : 64,\n",
    "'batch_size' : 512,\n",
    "'epoch' : 10,\n",
    "'gnn_layer_size' : 2,\n",
    "'patience' : 5,\n",
    "'save_flag' : 0,\n",
    "'dropout' : 0.5, \n",
    "'comment' : \"\",\n",
    "'lr_dc' : 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(res,labels):\n",
    "    # res: (bs, seq_len)\n",
    "    res = torch.concat(res)\n",
    "    acc_ar = (res == labels)\n",
    "    acc = acc_ar.sum()\n",
    "    \n",
    "    rank = torch.argmax(acc_ar, -1) +1\n",
    "    mrr = acc/rank.mean()\n",
    "    ndcg = (1/torch.log2(rank+1)).mean()\n",
    "    \n",
    "    return acc.mean(), mrr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk(config, model, test_dataset):\n",
    "    model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    res50 = []\n",
    "    res20 = []\n",
    "    res10 = []\n",
    "    res5  = []\n",
    "    label = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (uid, seq, mask, seq_len, pos_idx, labels) in tqdm(test_dataset):\n",
    "            \n",
    "            user = user.to(device)\n",
    "            seq = seq.to(device)\n",
    "            mask = mask.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            pos_idx = pos_idx.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            scores = model(uid, seq, mask, seq_len, pos_idx)\n",
    "            \n",
    "            res50.append(torch.topk(scores, 50)[1])\n",
    "            res20.append(torch.topk(scores, 20)[1])\n",
    "            res10.append(torch.topk(scores, 10)[1])\n",
    "            res5.append(torch.topk(scores, 5)[1])\n",
    "            label.append(labels)\n",
    "        \n",
    "        label = torch.concat(label)\n",
    "        \n",
    "        acc50, mrr50, ndcg50 = metrics(res50, labels)\n",
    "        acc20, mrr20, ndcg20 = metrics(res20, labels)\n",
    "        acc10, mrr10, ndcg10 = metrics(res10, labels)\n",
    "        acc5,  mrr5,  ndcg5 = metrics(res5, labels)\n",
    "        \n",
    "        print(\"Top50 : acc {} , mrr {}, ndcg {}\".format(acc50, mrr50, ndcg50))\n",
    "        print(\"Top20 : acc {} , mrr {}, ndcg {}\".format(acc20, mrr20, ndcg20))\n",
    "        print(\"Top10 : acc {} , mrr {}, ndcg {}\".format(acc10, mrr10, ndcg10))\n",
    "        print(\"Top5 : acc {} , mrr {}, ndcg {}\".format(acc5, mrr5, ndcg5))\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_dataset,test_dataset):\n",
    "    opimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(opimizer, step_size=config['epoch'], gamma=config['lr_dc'])\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print_epoch_step = 2\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    for epoch in range(config['epoch']):\n",
    "        t_loss = float(0)\n",
    "        model.train()\n",
    "        for i, data in tqdm(enumerate(train_dataset), desc='Epoch '+str(epoch)):\n",
    "            opimizer.zero_grad()\n",
    "            user, seq, mask, seq_len, pos_idx, labels = data\n",
    "            \n",
    "            user = user.to(device)\n",
    "            seq = seq.to(device)\n",
    "            mask = mask.to(device)\n",
    "            seq_len = seq_len.to(device)\n",
    "            pos_idx = pos_idx.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            scores = model(user, seq, mask, seq_len, pos_idx)\n",
    "            loss = criteria(scores, (labels-1).squeeze())\n",
    "            loss.backward()\n",
    "            opimizer.step()\n",
    "            t_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % print_epoch_step == 0:\n",
    "            print(f\"Epoch {epoch} - Loss : {t_loss/len(train_dataset)} \")\n",
    "            evaluate_topk(config, model, test_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HG_GNN(G, config, item_num, max_seq_len=10, max_sess=10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef354b4362334933b8706d912a41542c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DGLError",
     "evalue": "Cannot assign node feature \"h\" on device cuda:0 to a graph on device cpu. Call DGLGraph.to() to copy the graph to the same device.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, model, train_dataset, test_dataset)\u001b[0m\n\u001b[0;32m     20\u001b[0m pos_idx \u001b[38;5;241m=\u001b[39m pos_idx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 23\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(scores, (labels\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[33], line 89\u001b[0m, in \u001b[0;36mHG_GNN.forward\u001b[1;34m(self, user, seq, mask, seq_len, pos_idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m user \u001b[38;5;241m=\u001b[39m user \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_num\n\u001b[0;32m     88\u001b[0m v2e_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv2e(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mG\u001b[38;5;241m.\u001b[39mnumber_of_nodes())\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m---> 89\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv2e_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m h1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h1)\n\u001b[0;32m     92\u001b[0m bs \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\nn\\pytorch\\conv\\sageconv.py:234\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[1;34m(self, graph, feat, edge_weight)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# Message Passing\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggre_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 234\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrcdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_neigh(feat_src) \u001b[38;5;28;01mif\u001b[39;00m lin_before_mp \u001b[38;5;28;01melse\u001b[39;00m feat_src\n\u001b[0;32m    236\u001b[0m     )\n\u001b[0;32m    237\u001b[0m     graph\u001b[38;5;241m.\u001b[39mupdate_all(msg_fn, fn\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneigh\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    238\u001b[0m     h_neigh \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mdstdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneigh\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\view.py:99\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, (\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe HeteroNodeDataView has only one node type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease pass a tensor directly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     )\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_n_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ntid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dgl\\heterograph.py:4349\u001b[0m, in \u001b[0;36mDGLGraph._set_n_repr\u001b[1;34m(self, ntid, u, data)\u001b[0m\n\u001b[0;32m   4344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpect number of features to match number of nodes (len(u)).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nfeats, num_nodes)\n\u001b[0;32m   4347\u001b[0m     )\n\u001b[0;32m   4348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcontext(val) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[1;32m-> 4349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[0;32m   4350\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot assign node feature \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to a graph on\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   4351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Call DGLGraph.to() to copy the graph to the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key, F\u001b[38;5;241m.\u001b[39mcontext(val), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4353\u001b[0m     )\n\u001b[0;32m   4354\u001b[0m \u001b[38;5;66;03m# To prevent users from doing things like:\u001b[39;00m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;66;03m#     g.pin_memory_()\u001b[39;00m\n\u001b[0;32m   4357\u001b[0m \u001b[38;5;66;03m#     g.ndata['x'] = torch.randn(...)\u001b[39;00m\n\u001b[0;32m   4358\u001b[0m \u001b[38;5;66;03m#     sg = g.sample_neighbors(torch.LongTensor([...]).cuda())\u001b[39;00m\n\u001b[0;32m   4359\u001b[0m \u001b[38;5;66;03m#     sg.ndata['x']    # Becomes a CPU tensor even if sg is on GPU due to lazy slicing\u001b[39;00m\n\u001b[0;32m   4360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_pinned()\n\u001b[0;32m   4362\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcontext(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4363\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m F\u001b[38;5;241m.\u001b[39mis_pinned(val)\n\u001b[0;32m   4364\u001b[0m ):\n",
      "\u001b[1;31mDGLError\u001b[0m: Cannot assign node feature \"h\" on device cuda:0 to a graph on device cpu. Call DGLGraph.to() to copy the graph to the same device."
     ]
    }
   ],
   "source": [
    "train(config, model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
