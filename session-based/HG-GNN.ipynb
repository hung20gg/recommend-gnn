{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq dgl-cu110 dglgo -f https://data.dgl.ai/wheels/repo.html &>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "from pandas import Timedelta\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import dgl\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import dgl.function as FN\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "[Last-FM](https://www.kaggle.com/datasets/japarra27/lastfm-dataset)\n",
    "\n",
    "[Heterogeneous Global Graph Neural Networks for Personalized\n",
    "Session-based Recommendation](https://arxiv.org/pdf/2107.03813.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cudf.read_parquet('/data/items.parquet')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" # Unique User : {data['user_id'].nunique()}, # Unique Artist : {data['artist_id'].nunique()}, # Unique Track : {data['track_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create session with interval of 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_id(df, session):\n",
    "    # If the next row have different user_id or the time difference is greater than session, then it is a new session\n",
    "    df_prev = df.shift(1)\n",
    "    is_new_session = (df['user_id'] != df_prev['user_id']) | (df['timestamp'] - df_prev['timestamp'] > session)\n",
    "    session_id = is_new_session.cumsum()-1\n",
    "    return session_id\n",
    "\n",
    "def group_session(df, session):\n",
    "    df['session_id'] = get_session_id(df, session)\n",
    "    return df\n",
    "\n",
    "def filter_short_session(df, min_session_length=2):\n",
    "    session_length = df.groupby('session_id').size()\n",
    "    session_length = session_length[session_length >= min_session_length]\n",
    "    return df[df['session_id'].isin(session_length.index)]\n",
    "\n",
    "def filter_infrequent_item(df, min_item_support=5):\n",
    "    item_support = df.groupby('item_id').size()\n",
    "    item_support = item_support[item_support >= min_item_support]\n",
    "    return df[df['item_id'].isin(item_support.index)]\n",
    "\n",
    "def filter_until_ok(df, min_session_length=2, min_item_support=5):\n",
    "    while True:\n",
    "        before = len(df)\n",
    "        df = filter_short_session(df, min_session_length)\n",
    "        df = filter_infrequent_item(df, min_item_support)\n",
    "        after = len(df)\n",
    "        if before == after:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def trucate_session(df, session_length=20, is_sorted=True):\n",
    "    if not is_sorted:\n",
    "        df = df.sort_values(['session_id', 'timestamp'])\n",
    "    item_idx = df.groupby('session_id').cumcount()\n",
    "    return df[item_idx < session_length]\n",
    "\n",
    "def update_id(df, field):\n",
    "    labels = cudf.factorize(df[field])[0]\n",
    "    kwargs = {field: labels}\n",
    "    df = df.assign(**kwargs)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_immediate_repeats(df):\n",
    "    df_prev = df.shift()\n",
    "    is_not_repeat = (df['session_id'] != df_prev['session_id']) | (df['item_id'] != df_prev['item_id'])\n",
    "    return df[is_not_repeat]\n",
    "    \n",
    "def reorder_sessions(df):\n",
    "    df_endtime  = df.groupby('session_id')['timestamp'].max().sort_value().reset_index()\n",
    "    oid2nid = dict(zip(df_endtime['session_id'], df_endtime.index))\n",
    "    df['session_id'].map(oid2nid, inplace=True)\n",
    "    df.sort_values(['session_id', 'timestamp'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def keep_top_n_items(df, n=40000):\n",
    "    item_support = df.groupby('item_id').size()\n",
    "    top_n_items = item_support.nlargest(n).index\n",
    "    return df[df['item_id'].isin(top_n_items)]\n",
    "\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    endtime  = df.groupby('session_id')['timestamp'].max().sort_values()\n",
    "    num_test = int(len(endtime) * test_size)\n",
    "    test_sessions = endtime.index[-num_test:]\n",
    "    df_train = df[~df['session_id'].isin(test_sessions)]\n",
    "    df_test = df[df['session_id'].isin(test_sessions)]\n",
    "    return df_train, df_test\n",
    "    \n",
    "def save_sessions(df, filepath='data/sessions.csv'):\n",
    "    df = reorder_sessions(df)\n",
    "    sessions = df.groupby('session_id').itemId.apply(lambda x: ','.join(map(str, x)))\n",
    "    sessions.to_csv(filepath, sep='\\t', header=False, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = Timedelta(hours=6)\n",
    "n = 40000\n",
    "\n",
    "data = data[['user_id', 'artist_id', 'timestamp']]\n",
    "data.columns = ['user_id', 'item_id', 'timestamp']\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data = update_id(data, 'user_id')\n",
    "data = update_id(data, 'item_id')\n",
    "\n",
    "data.sort_values(['user_id', 'timestamp'], inplace=True)\n",
    "data = group_session(data, interval)\n",
    "\n",
    "data = remove_immediate_repeats(data)\n",
    "data = trucate_session(data, 20)\n",
    "\n",
    "data = keep_top_n_items(data, n)\n",
    "data = filter_until_ok(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"#Users : {data['user_id'].nunique()} #Items : {data['item_id'].nunique()} #Sessions : {data['session_id'].nunique()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/data.csv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _aggregate_session(df):\n",
    "    res = []\n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        res += group.groupby('session_id')['item_id'].agg(list).tolist()\n",
    "    return res\n",
    "\n",
    "def _aggregate_df(df):\n",
    "    res = dict()\n",
    "    for uid, group in df.groupby('user_id'):\n",
    "        res[uid] = group.groupby('session_id')['item_id'].agg(list).tolist()\n",
    "        \n",
    "def split_data(val_ratio = 0.2, test_ratio = 0.2):\n",
    "    data = pd.read_csv('data/data.csv', sep='\\t', names=['user_id', 'item_id', 'timestamp'])\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    \n",
    "    df_train, df_test = train_test_split(data, test_size=test_ratio)\n",
    "    \n",
    "    df_test = df_test[df_test['item_id'].isin(df_train['item_id'].unique())]\n",
    "    df_test = filter_short_session(df_test)\n",
    "\n",
    "    print(f'No. of Clicks: {len(df_train) + len(df_test)}')\n",
    "    print(f'No. of Items: {df_train['item_id'].nunique()}')\n",
    "\n",
    "    # update itemId\n",
    "    train_itemId_new, uniques = pd.factorize(df_train['item_id'])\n",
    "    df_train = df_train.assign(item_id=train_itemId_new)\n",
    "    oid2nid = {oid: i for i, oid in enumerate(uniques)}\n",
    "    test_itemId_new = df_test['item_id'].map(oid2nid)\n",
    "    df_test = df_test.assign(item_id=test_itemId_new)\n",
    "    \n",
    "    df_train['user_id']+=1\n",
    "    df_train['item_id']+=1\n",
    "    df_test['user_id']+=1\n",
    "    df_test['item_id']+=1\n",
    "    \n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_val = df_test.sample(frac=val_ratio, random_state=42)\n",
    "    part_test = df_test[~df_test.index.isin(df_val.index)]\n",
    "    \n",
    "    with open('data/train.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_train), f)\n",
    "        \n",
    "    with open('data/val.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_val), f)\n",
    "        \n",
    "    with open('data/test.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(part_test), f)\n",
    "        \n",
    "    with open('data/all_test.pkl', 'wb') as f:\n",
    "        pickle.dump(_aggregate_df(df_test), f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 12\n",
    "SEQ_LEN = 10 ## Window Size to create a training Sequence\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_seq(data_list):\n",
    "    # final_seqs = [(user_id, seq, [next_item])]\n",
    "    uid = []\n",
    "\n",
    "    masks = []\n",
    "    labels =[]\n",
    "    browsed_ids = []\n",
    "    temp_browsed_id = [0 for _ in SEQ_LEN]\n",
    "    pos_idx = []\n",
    "    seq_lens = []\n",
    "    \n",
    "    final_seq = []\n",
    "    \n",
    "    for u in tqdm(data_list):\n",
    "        u_seqs = data_list[u]\n",
    "        for seq in u_seqs:\n",
    "            for i in range(1, len(seq)):\n",
    "                \n",
    "                temp_seq = seq[-i-SEQ_LEN:-i]\n",
    "                len_seq = len(temp_seq)\n",
    "                mask = len_seq + [0]*(SEQ_LEN-len_seq)\n",
    "                pos_idx = [len_seq-1-i for i in range(len_seq)]+ [0]*(SEQ_LEN-len_seq)\n",
    "                browsed_id = temp_browsed_id.copy()\n",
    "                browsed_id[:len_seq] = temp_seq\n",
    "                \n",
    "                masks.append(mask)\n",
    "                pos_idx.append(pos_idx)\n",
    "                browsed_ids.append(browsed_id)\n",
    "                labels.append([int(seq[-i])])\n",
    "                uid.append([int(u)])\n",
    "                seq_lens.append(len_seq)\n",
    "                \n",
    "                final_seq.append((u, seq[:-i], [seq[-i]]))\n",
    "        \n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    uid = torch.tensor(uid, dtype=torch.long)\n",
    "    masks = torch.tensor(masks, dtype=torch.bool)  \n",
    "    browsed_ids = torch.tensor(browsed_ids, dtype=torch.long)\n",
    "    pos_idx = torch.tensor(pos_idx, dtype=torch.long)\n",
    "    seq_lens = torch.tensor(seq_lens, dtype=torch.long)\n",
    "        \n",
    "    return final_seq, (uid, browsed_ids, masks,seq_lens, pos_idx, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open('data/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "test_seq, test_data = common_seq(test)\n",
    "train_seq, train_data = common_seq(train)\n",
    "\n",
    "# with open('data/train_seq.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_seq, f)\n",
    "# with open('data/test_seq.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_seq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(\n",
    "    dataset=TensorDataset(*train_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_iter = DataLoader(\n",
    "    dataset=TensorDataset(*test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\" Length of Training DataLoader is {len(train_iter)} & Test DataLoader {len(test_iter)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for an iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = next(iter(train_iter))\n",
    "print(f\"Number of elements in the example tuple {len(eg)} - corresponds to uid | browsed_ids | mask | seq_len | label | pos_idx \")\n",
    "\n",
    "print(\"Size of - \")\n",
    "print(f\" User ID Tensor - {eg[0].size()} \") \n",
    "print(f\" Sequence of Items - {eg[1].size()} \") \n",
    "print(f\" Sequence of Masks - {eg[2].size()} \") \n",
    "print(f\" Actual Sequence Length (before padding) - {eg[3].size()} \") \n",
    "print(f\" Labels - {eg[4].size()} \") \n",
    "print(f\" Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - {eg[5].size()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual tensor of - \")\n",
    "print(f\" User ID - {eg[0][0]} \") \n",
    "print(f\" Sequence of Items - {eg[0][1]} \") \n",
    "print(f\" Sequence of Masks - {eg[0][2]} \") \n",
    "print(f\" Actual Sequence Length (before padding) - {eg[0][3]} \") \n",
    "print(f\" Labels - {eg[0][4]} \") \n",
    "print(f\" Position Index (Oldest Index = 0, Latest Index can be till 9, Padded Index = 0) - {eg[0][5]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Heterogenous Global Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we have to create 4 type of edges\n",
    "* In and out connection (the prev node and the forward node) of a item in session\n",
    "* User similarity\n",
    "* Item similarity (based on session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_relation(num, sample_size=20):\n",
    "    \n",
    "    adj1 = [dict() for _ in range(num)]\n",
    "    adj2 = [dict() for _ in range(num)]\n",
    "    adj_in = [[] for _ in range(num)]\n",
    "    adj_out = [[] for _ in range(num)]\n",
    "\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    \n",
    "    for u in tqdm(graph):\n",
    "        u_seqs = graph[u]\n",
    "        for seq in u_seqs:\n",
    "            for i in range(1, len(seq)):\n",
    "                if seq[i] not in adj1[seq[i-1]]:\n",
    "                    adj1[seq[i-1]][seq[i]] = 1\n",
    "                else:\n",
    "                    adj1[seq[i-1]][seq[i]] += 1\n",
    "\n",
    "                if seq[i-1] not in adj2[seq[i]]:\n",
    "                    adj2[seq[i]][seq[i-1]] = 1\n",
    "                else:\n",
    "                    adj2[seq[i]][seq[i-1]] += 1\n",
    "                    \n",
    "    weights = [[] for _ in range(num)]\n",
    "    \n",
    "    for t in range(1, num):\n",
    "        x = [v for v in sorted(adj1[t].items(), reverse=True, key=lambda x: x[1])]\n",
    "        adj_out[t] = [v[0] for v in x]\n",
    "\n",
    "    for t in range(1, num):\n",
    "        x = [v for v in sorted(adj2[t].items(), reverse=True, key=lambda x: x[1])]\n",
    "        adj_in[t] = [v[0] for v in x]\n",
    "\n",
    "    # edge sampling \n",
    "    for i in range(1, num):\n",
    "        adj_in[i] = adj_in[i][:sample_size]\n",
    "    for i in range(1, num):\n",
    "        adj_out[i] = adj_out[i][:sample_size]\n",
    "        \n",
    "    print(f\"Items which most frequently lies to the left (previous time step) of Item 1 : {adj_in[1]} \")\n",
    "    print(f\"Items which most frequently lies to the right (next time step) of Item 1 : {adj_out[1]} \")\n",
    "        \n",
    "    with open('data/adj_in.pkl', 'wb') as f:\n",
    "        pickle.dump(adj_in, f)\n",
    "    \n",
    "    with open('data/adj_out.pkl', 'wb') as f:\n",
    "        pickle.dump(adj_out, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userCF(K=100):\n",
    "    \n",
    "    vid_user = {}\n",
    "    user_sim_matrix ={}\n",
    "    uid_vcount = {}\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        session_data = pickle.load(f)\n",
    "        \n",
    "    for uid in tqdm(session_data):\n",
    "        for seq in session_data[uid]:\n",
    "            uid_vcount[uid] = set()\n",
    "            for vid in seq:\n",
    "                if vid not in vid_user:\n",
    "                    vid_user[vid] = set()\n",
    "                vid_user[vid].add(uid)\n",
    "                uid_vcount[uid].add(vid)\n",
    "                \n",
    "    for vid, users in tqdm(vid_user.items()):\n",
    "        for u in users:\n",
    "            if u not in user_sim_matrix:\n",
    "                user_sim_matrix[u] = dict()\n",
    "            for v in users:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                if v not in user_sim_matrix[u]:\n",
    "                    user_sim_matrix[u][v] = 0\n",
    "                user_sim_matrix[u][v] += 1/len(users)\n",
    "                \n",
    "    for u, related_users in tqdm(user_sim_matrix.items()):\n",
    "        for v, count in related_users.items():\n",
    "            user_sim_matrix[u][v] = count / math.sqrt(len(uid_vcount[u]) * len(uid_vcount[v]))\n",
    "    \n",
    "    user_topK = {}     \n",
    "            \n",
    "    for user in user_sim_matrix:\n",
    "        user_topK[user] = sorted(user_sim_matrix[user].items(), key=itemgetter(1), reverse=True)[:K]\n",
    "        \n",
    "    with open('data/user_sim_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(user_topK, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemCF_by_Session(K=200):\n",
    "    \"\"\"\n",
    "        calculate item similarity matrix by session (kinda like TF-IDF)\n",
    "    \"\"\"\n",
    "    \n",
    "    sess_item = {}\n",
    "    item_sim_matrix = {}\n",
    "    vid_ucount = {}\n",
    "    sess_cnt = 0\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        session_data = pickle.load(f)\n",
    "        \n",
    "    for uid in tqdm(session_data):\n",
    "        for seq in session_data[uid]:\n",
    "            sess_cnt += 1\n",
    "            sess_item[sess_cnt] = set()\n",
    "            for vid in seq:\n",
    "                sess_item[sess_cnt].add(vid)\n",
    "                if vid not in vid_ucount:\n",
    "                    vid_ucount[vid] = set()\n",
    "                vid_ucount[vid].add(sess_cnt)\n",
    "    \n",
    "    for sess, items in tqdm(sess_item.items()):\n",
    "        for u in items:\n",
    "            if u not in item_sim_matrix:\n",
    "                item_sim_matrix[u] = dict()\n",
    "            for v in items:\n",
    "                if u == v:\n",
    "                    continue\n",
    "                if v not in item_sim_matrix[u]:\n",
    "                    item_sim_matrix[u][v] = 0\n",
    "                item_sim_matrix[u][v] += 1/len(items)\n",
    "                \n",
    "    for u, related_items in tqdm(item_sim_matrix.items()):\n",
    "        for v, count in related_items.items():\n",
    "            item_sim_matrix[u][v] = count / math.sqrt(len(vid_ucount[u]) * len(vid_ucount[v]))\n",
    "            \n",
    "    item_topK = {}\n",
    "    for item in item_sim_matrix:\n",
    "        item_topK[item] = sorted(item_sim_matrix[item].items(), key=itemgetter(1), reverse=True)[:K]\n",
    "        \n",
    "    with open('data/item_sim_matrix.pkl', 'wb') as f:\n",
    "        pickle.dump(item_topK, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_relation()\n",
    "userCF()\n",
    "itemCF_by_Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Heterogenous Global Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uui_graph(sample_size, topK,add_u=True, add_v=True):\n",
    "    \n",
    "    with open('data/train.pkl', 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    with open('data/adj_in.pkl', 'rb') as f:\n",
    "        adj_in = pickle.load(f)\n",
    "    with open('data/adj_out.pkl', 'rb') as f:\n",
    "        adj_out = pickle.load(f)\n",
    "    with open('data/user_sim_matrix.pkl', 'rb') as f:\n",
    "        user_sim_matrix = pickle.load(f)\n",
    "    with open('data/item_sim_matrix.pkl', 'rb') as f:\n",
    "        item_sim_matrix = pickle.load(f)\n",
    "        \n",
    "    pre = []\n",
    "    nxt = []\n",
    "    src_v = []\n",
    "    dst_u = []\n",
    "    \n",
    "    for i in range(1, len(adj_in)):\n",
    "        _pre = []\n",
    "        _nxt = []\n",
    "        \n",
    "        for j in adj_in[i]:\n",
    "            _pre.append(i)\n",
    "            _nxt.append(j)\n",
    "        \n",
    "        pre += _pre\n",
    "        nxt += _nxt\n",
    "    \n",
    "    o_pre = []\n",
    "    o_nxt = []\n",
    "    \n",
    "    for i in range(1, len(adj_out)):\n",
    "        _pre = []\n",
    "        _nxt = []\n",
    "        \n",
    "        for j in adj_out[i]:\n",
    "            _pre.append(i)\n",
    "            _nxt.append(j)\n",
    "        \n",
    "        o_pre += _pre\n",
    "        o_nxt += _nxt\n",
    "        \n",
    "    for u in tqdm(graph):\n",
    "        for seq in graph[u]:\n",
    "            pre += seq[:-1]\n",
    "            nxt += seq[1:]\n",
    "            dst_u += [u for _ in seq]\n",
    "            src_v += seq\n",
    "            \n",
    "    topv_src = []\n",
    "    topv_dst = []\n",
    "    \n",
    "    for v in tqdm(item_sim_matrix):\n",
    "        tmp_src =[]\n",
    "        tmp_dst =[]\n",
    "        \n",
    "        exclusion = adj_in[v] + adj_out[v]\n",
    "        for vid, value in item_sim_matrix[v][:topK][:int(len(exclusion))]:\n",
    "            if vid not in exclusion:\n",
    "                tmp_src.append(v)\n",
    "                tmp_dst.append(vid)\n",
    "                \n",
    "        topv_src += tmp_src\n",
    "        topv_dst += tmp_dst\n",
    "        \n",
    "    u_src = []\n",
    "    u_dst = []\n",
    "    \n",
    "    for u in tqdm(user_sim_matrix):\n",
    "        tmp_src =[]\n",
    "        tmp_dst =[]\n",
    "        \n",
    "        for uid, value in user_sim_matrix[u][:topK]:\n",
    "            tmp_src.append(u)\n",
    "            tmp_dst.append(uid)\n",
    "                \n",
    "        u_src += tmp_src\n",
    "        u_dst += tmp_dst\n",
    "        \n",
    "    item_num = max(max(pre), max(nxt)) + 1\n",
    "    user_num = max(max(u_src), max(u_dst)) \n",
    "    \n",
    "    u_src = [i+item_num for i in u_src]\n",
    "    u_dst = [i+item_num for i in u_dst]\n",
    "    dst_u = [i+item_num for i in dst_u]\n",
    "    \n",
    "    G = dgl.graph((pre,nxt))\n",
    "    G.add_edges(nxt, pre)\n",
    "    G.add_edges(dst_u, src_v)\n",
    "    G.add_edges(src_v, dst_u)\n",
    "    \n",
    "    if add_u:\n",
    "        G.add_edges(u_src, u_dst)\n",
    "        G.add_edges(u_dst, u_src)\n",
    "        \n",
    "    if add_v:\n",
    "        G.add_edges(topv_src, topv_dst)\n",
    "        G.add_edges(topv_dst, topv_src)\n",
    "        \n",
    "    G = dgl.add_self_loop(G)\n",
    "    \n",
    "    return G, item_num,pre, nxt, dst_u, src_v, u_src, u_dst, topv_src, topv_dst   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G,item_num, pre, nxt, dst_u, src_v, u_src, u_dst, topv_src, topv_dst = uui_graph(SZ, topK = 20, add_u = True, add_v = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HG_GNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
